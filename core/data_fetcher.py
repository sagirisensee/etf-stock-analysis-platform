import akshare as ak
import pandas as pd
from cachetools import cached, TTLCache
import os
import logging
import asyncio
from tenacity import retry, stop_after_attempt, wait_exponential, wait_fixed
import sqlite3
from contextlib import contextmanager
import time
import random
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

# ä»ç¯å¢ƒå˜é‡è·å–ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼Œé»˜è®¤60ç§’
CACHE_EXPIRE = int(os.getenv('CACHE_EXPIRE_SECONDS', '60')) 
cache = TTLCache(maxsize=10, ttl=CACHE_EXPIRE)

# æ•°æ®åº“è·¯å¾„
DB_PATH = 'etf_analysis.db'

# åçˆ¬è™«æ§åˆ¶
class AntiCrawlingController:
    """åçˆ¬è™«æ§åˆ¶å™¨"""
    def __init__(self):
        self.last_request_time = {}
        self.request_count = {}
        self.base_delay = 6  # åŸºç¡€å»¶è¿Ÿ6ç§’ï¼ˆå»¶é•¿ä¸€å€ï¼‰
        self.max_delay = 20  # æœ€å¤§å»¶è¿Ÿ20ç§’ï¼ˆå»¶é•¿ä¸€å€ï¼‰
        self.request_window = 60  # è¯·æ±‚æ—¶é—´çª—å£ï¼ˆç§’ï¼‰
        self.max_requests_per_window = 10  # æ¯ä¸ªæ—¶é—´çª—å£æœ€å¤§è¯·æ±‚æ•°
    
    def get_smart_delay(self, api_name: str) -> float:
        """è·å–æ™ºèƒ½å»¶è¿Ÿæ—¶é—´"""
        current_time = time.time()
        
        # æ¸…ç†è¿‡æœŸçš„è¯·æ±‚è®°å½•
        if api_name in self.last_request_time:
            if current_time - self.last_request_time[api_name] > self.request_window:
                self.request_count[api_name] = 0
        
        # è®¡ç®—å»¶è¿Ÿ
        if api_name not in self.request_count:
            self.request_count[api_name] = 0
        
        # æ ¹æ®è¯·æ±‚é¢‘ç‡åŠ¨æ€è°ƒæ•´å»¶è¿Ÿ
        recent_requests = self.request_count.get(api_name, 0)
        if recent_requests >= self.max_requests_per_window:
            # è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œå¢åŠ å»¶è¿Ÿ
            delay = self.base_delay + random.uniform(2, 5)
        elif recent_requests >= self.max_requests_per_window * 0.7:
            # è¯·æ±‚è¾ƒå¤šï¼Œé€‚åº¦å¢åŠ å»¶è¿Ÿ
            delay = self.base_delay + random.uniform(1, 3)
        else:
            # æ­£å¸¸è¯·æ±‚ï¼Œä½¿ç”¨åŸºç¡€å»¶è¿Ÿ
            delay = self.base_delay + random.uniform(0, 2)
        
        # ç¡®ä¿å»¶è¿Ÿåœ¨åˆç†èŒƒå›´å†…
        delay = min(max(delay, self.base_delay), self.max_delay)
        
        # æ™ºèƒ½å»¶è¿Ÿæ§åˆ¶å·²å¯ç”¨
        return delay
    
    def record_request(self, api_name: str):
        """è®°å½•è¯·æ±‚"""
        current_time = time.time()
        self.last_request_time[api_name] = current_time
        self.request_count[api_name] = self.request_count.get(api_name, 0) + 1

# å…¨å±€åçˆ¬è™«æ§åˆ¶å™¨
anti_crawling = AntiCrawlingController()

# å†å²æ•°æ®è·å–é…ç½®
class DataConfig:
    """æ•°æ®è·å–é…ç½®"""
    def __init__(self):
        # æ ¹æ®éœ€è¦çš„æŠ€æœ¯æŒ‡æ ‡è°ƒæ•´å¤©æ•°
        # å½“å‰ä½¿ç”¨: SMA_5, SMA_10, SMA_20, SMA_60, MACD(26), å¸ƒæ—å¸¦(20)
        # ä¸ºäº†åˆ¤æ–­60æ—¥å‡çº¿è¶‹åŠ¿ï¼Œéœ€è¦è‡³å°‘61å¤©æ•°æ®ï¼ˆå½“å‰å¤© + å‰ä¸€å¤©ï¼‰
        self.max_days = int(os.getenv('HISTORY_DATA_DAYS', '120'))  # é»˜è®¤120å¤©ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®åˆ¤æ–­60æ—¥å‡çº¿è¶‹åŠ¿
        self.min_days = 61  # æœ€å°‘61å¤©ï¼Œä¿è¯60æ—¥å‡çº¿è¶‹åŠ¿åˆ¤æ–­
    
    def get_date_range(self):
        """è·å–æ•°æ®æ—¥æœŸèŒƒå›´"""
        end_date = datetime.now().strftime('%Y%m%d')
        start_date = (datetime.now() - timedelta(days=self.max_days)).strftime('%Y%m%d')
        return start_date, end_date

# å…¨å±€æ•°æ®é…ç½®
data_config = DataConfig()

@contextmanager
def get_db():
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    try:
        yield conn
    finally:
        conn.close()

def get_stock_pools_from_db(pool_type=None):
    """ä»æ•°æ®åº“è·å–æ ‡çš„æ± """
    try:
        with get_db() as conn:
            if pool_type:
                results = conn.execute(
                    'SELECT * FROM stock_pools WHERE type = ? ORDER BY name',
                    (pool_type,)
                ).fetchall()
            else:
                results = conn.execute(
                    'SELECT * FROM stock_pools ORDER BY type, name'
                ).fetchall()
            return [dict(row) for row in results]
    except Exception as e:
        logger.error(f"ä»æ•°æ®åº“è·å–æ ‡çš„æ± å¤±è´¥: {e}")
        return []

@cached(cache)
def get_all_etf_spot_realtime():
    """è·å–æ‰€æœ‰ETFçš„å®æ—¶è¡Œæƒ…æ•°æ® (å¸¦ç¼“å­˜å’Œå¤šæ•°æ®æº)"""
    logger.info("æ­£åœ¨ä»AKShareè·å–æ‰€æœ‰ETFå®æ—¶æ•°æ®...(ç¼“å­˜æœ‰æ•ˆæœŸ: %sç§’)", CACHE_EXPIRE)
    
    # æ•°æ®æºåˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
    etf_data_sources = [
        {
            'name': 'fund_etf_spot_em',
            'func': ak.fund_etf_spot_em,
            'description': 'ä¸œæ–¹è´¢å¯ŒETFå®æ—¶æ•°æ®'
        },
        {
            'name': 'fund_etf_spot_ths', 
            'func': ak.fund_etf_spot_ths,
            'description': 'åŒèŠ±é¡ºETFå®æ—¶æ•°æ®'
        }
    ]
    
    for source in etf_data_sources:
        try:
            # åº”ç”¨æ™ºèƒ½å»¶è¿Ÿæ§åˆ¶
            delay = anti_crawling.get_smart_delay(source['name'])
            time.sleep(delay)
            
            # å°è¯•æ•°æ®æº
            df = source['func']()
            
            # è®°å½•æˆåŠŸè¯·æ±‚
            anti_crawling.record_request(source['name'])
            # æ•°æ®è·å–æˆåŠŸ
            
            # æ ¹æ®æ•°æ®æºè¿›è¡Œä¸åŒçš„åˆ—åæ˜ å°„
            if source['name'] == 'fund_etf_spot_em':
                # ä¸œæ–¹è´¢å¯Œæ•°æ®æº
                column_mapping = {
                    'ä»£ç ': 'ä»£ç ',
                    'åç§°': 'åç§°',
                    'æœ€æ–°ä»·': 'æœ€æ–°ä»·',
                    'æ¶¨è·Œå¹…': 'æ¶¨è·Œå¹…',
                    'æ¶¨è·Œé¢': 'æ¶¨è·Œé¢',
                    'æ˜¨æ”¶': 'æ˜¨æ”¶'
                }
            elif source['name'] == 'fund_etf_spot_ths':
                # åŒèŠ±é¡ºæ•°æ®æº
                column_mapping = {
                    'åŸºé‡‘ä»£ç ': 'ä»£ç ',
                    'åŸºé‡‘åç§°': 'åç§°',
                    'å½“å‰-å•ä½å‡€å€¼': 'æœ€æ–°ä»·',
                    'å¢é•¿ç‡': 'æ¶¨è·Œå¹…',
                    'å¢é•¿å€¼': 'æ¶¨è·Œé¢',
                    'å‰ä¸€æ—¥-å•ä½å‡€å€¼': 'æ˜¨æ”¶'
                }
            else:
                # é»˜è®¤æ˜ å°„
                column_mapping = {
                    'ä»£ç ': 'ä»£ç ',
                    'åç§°': 'åç§°',
                    'æœ€æ–°ä»·': 'æœ€æ–°ä»·',
                    'æ¶¨è·Œå¹…': 'æ¶¨è·Œå¹…',
                    'æ¶¨è·Œé¢': 'æ¶¨è·Œé¢',
                    'æ˜¨æ”¶': 'æ˜¨æ”¶'
                }
            
            # é‡å‘½ååˆ—
            df = df.rename(columns=column_mapping)
            
            # æ ¹æ®æ•°æ®æºç¡®å®šéœ€è¦å¤„ç†çš„æ•°å€¼åˆ—
            if source['name'] == 'fund_etf_spot_ths':
                # åŒèŠ±é¡ºæ•°æ®æºæ²¡æœ‰æˆäº¤é¢ï¼Œåªå¤„ç†å…¶ä»–åˆ—
                numeric_cols = ['æœ€æ–°ä»·', 'æ˜¨æ”¶']
            else:
                # å…¶ä»–æ•°æ®æº
                numeric_cols = ['æœ€æ–°ä»·', 'æ˜¨æ”¶', 'æˆäº¤é¢']
            
            for col in numeric_cols:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # åªå¯¹å­˜åœ¨çš„åˆ—è¿›è¡Œdropna
            available_cols = [col for col in numeric_cols if col in df.columns]
            df.dropna(subset=available_cols, inplace=True)
            # ETFæ¶¨è·Œå¹…å¤„ç†ï¼šETFç±»å‹ï¼Œå¿…é¡»ä¹˜ä»¥100è½¬æ¢ä¸ºç™¾åˆ†æ¯”
            if 'æ¶¨è·Œå¹…' in df.columns:
                df['æ¶¨è·Œå¹…'] = pd.to_numeric(df['æ¶¨è·Œå¹…'], errors='coerce')
                # ETFæ•°æ®æºè¿”å›å°æ•°å½¢å¼ï¼Œå¿…é¡»è½¬æ¢ä¸ºç™¾åˆ†æ¯”
                df['æ¶¨è·Œå¹…'] = df['æ¶¨è·Œå¹…'] * 100
                logger.info("ğŸ”„ [ETFå®æ—¶æ•°æ®] æ¶¨è·Œå¹…ä»å°æ•°è½¬æ¢ä¸ºç™¾åˆ†æ¯”")
            else:
                # å¦‚æœæ²¡æœ‰æ¶¨è·Œå¹…åˆ—ï¼Œåˆ™è®¡ç®—
                df['æ¶¨è·Œå¹…'] = 0.0
                mask = df['æ˜¨æ”¶'] != 0
                df.loc[mask, 'æ¶¨è·Œå¹…'] = ((df.loc[mask, 'æœ€æ–°ä»·'] - df.loc[mask, 'æ˜¨æ”¶']) / df.loc[mask, 'æ˜¨æ”¶']) * 100
            return df
            
        except Exception as e:
            logger.warning(f"âš ï¸ [ETFå®æ—¶æ•°æ®] {source['description']} è·å–å¤±è´¥: {e}")
            continue  # å°è¯•ä¸‹ä¸€ä¸ªæ•°æ®æº
    
    # æ‰€æœ‰æ•°æ®æºéƒ½å¤±è´¥
    logger.error(f"ğŸ’¥ [ETFå®æ—¶æ•°æ®] æ‰€æœ‰æ•°æ®æºéƒ½è·å–å¤±è´¥")
    return None

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=2, min=8, max=120))
async def get_etf_daily_history(etf_code: str, data_type: str = "etf"):
    """è·å–å•æ”¯ETFçš„å†å²æ—¥çº¿æ•°æ® (å¸¦è‡ªåŠ¨é‡è¯•)"""
    logger.info(f"ğŸ” [ETFå†å²æ•°æ®] æ­£åœ¨è·å– {etf_code} çš„å†å²æ—¥çº¿æ•°æ®ï¼Œç±»å‹: {data_type}")
    try:
        # åº”ç”¨æ™ºèƒ½å»¶è¿Ÿæ§åˆ¶
        api_name = f"fund_etf_hist_em_{etf_code}"
        delay = anti_crawling.get_smart_delay(api_name)
        await asyncio.sleep(delay)
        
        # è·å–é…ç½®çš„æ—¥æœŸèŒƒå›´
        start_date, end_date = data_config.get_date_range()
        # è°ƒç”¨å†å²æ•°æ®æ¥å£
        # æ•°æ®èŒƒå›´é…ç½®
        
        daily_df = await asyncio.to_thread(
            ak.fund_etf_hist_em,
            symbol=etf_code,
            period="daily",
            adjust="qfq",
            start_date=start_date,
            end_date=end_date
        )
        
        # è®°å½•æˆåŠŸè¯·æ±‚
        anti_crawling.record_request(api_name)
        # å†å²æ•°æ®è·å–å®Œæˆ
        
        # æ ‡å‡†åŒ–åˆ—å
        if 'æ”¶ç›˜' in daily_df.columns:
            daily_df.rename(columns={'æ”¶ç›˜': 'close'}, inplace=True)
        if 'æœ€é«˜' in daily_df.columns:
            daily_df.rename(columns={'æœ€é«˜': 'high'}, inplace=True)
        if 'æœ€ä½' in daily_df.columns:
            daily_df.rename(columns={'æœ€ä½': 'low'}, inplace=True)
        if 'æ—¥æœŸ' in daily_df.columns:
            daily_df.rename(columns={'æ—¥æœŸ': 'date'}, inplace=True)
        
        # æ•°æ®å¤„ç†å®Œæˆ
        return daily_df
    except Exception as e:
        logger.error(f"ğŸ’¥ [ETFå†å²æ•°æ®] è·å– {etf_code} æ—¥çº¿æ•°æ®æ—¶å‡ºé”™ (å°†è¿›è¡Œé‡è¯•): {e}", exc_info=True)
        raise e

@cached(cache)
def get_all_stock_spot_realtime():
    """è·å–æ‰€æœ‰Aè‚¡çš„å®æ—¶è¡Œæƒ…æ•°æ® (å¸¦ç¼“å­˜)"""
    logger.info("æ­£åœ¨ä»AKShareè·å–æ‰€æœ‰Aè‚¡å®æ—¶æ•°æ®...(ç¼“å­˜æœ‰æ•ˆæœŸ: %sç§’)", CACHE_EXPIRE)
    try:
        # åº”ç”¨æ™ºèƒ½å»¶è¿Ÿæ§åˆ¶
        api_name = "stock_zh_a_spot_em"
        delay = anti_crawling.get_smart_delay(api_name)
        logger.info(f"â±ï¸ [è‚¡ç¥¨å®æ—¶æ•°æ®] å»¶è¿Ÿ {delay:.2f} ç§’åå¼€å§‹è·å–æ•°æ®")
        time.sleep(delay)
        
        # ä½¿ç”¨ä¸“é—¨è·å–è‚¡ç¥¨å®æ—¶è¡Œæƒ…çš„æ¥å£
        # è°ƒç”¨è‚¡ç¥¨å®æ—¶æ•°æ®æ¥å£
        logger.info("ğŸ“¡ [è‚¡ç¥¨å®æ—¶æ•°æ®] æ­£åœ¨è°ƒç”¨ ak.stock_zh_a_spot_em()")
        df = ak.stock_zh_a_spot_em()
        logger.info(f"âœ… [è‚¡ç¥¨å®æ—¶æ•°æ®] åŸå§‹æ•°æ®è·å–æˆåŠŸï¼Œå½¢çŠ¶: {df.shape}")
        
        # è®°å½•æˆåŠŸè¯·æ±‚
        anti_crawling.record_request(api_name)
        
        # æ ‡å‡†åŒ–åˆ—åæ˜ å°„
        column_mapping = {
            'ä»£ç ': 'ä»£ç ',
            'åç§°': 'åç§°',
            'æœ€æ–°ä»·': 'æœ€æ–°ä»·',
            'æ¶¨è·Œå¹…': 'æ¶¨è·Œå¹…',
            'æ¶¨è·Œé¢': 'æ¶¨è·Œé¢',
            'æ˜¨æ”¶': 'æ˜¨æ”¶'
        }
        
        # é‡å‘½ååˆ—
        logger.info(f"ğŸ“‹ [è‚¡ç¥¨å®æ—¶æ•°æ®] åŸå§‹åˆ—å: {list(df.columns)}")
        df = df.rename(columns=column_mapping)
        logger.info(f"ğŸ“‹ [è‚¡ç¥¨å®æ—¶æ•°æ®] é‡å‘½åååˆ—å: {list(df.columns)}")
        
        numeric_cols = ['æœ€æ–°ä»·', 'æ˜¨æ”¶', 'æˆäº¤é¢']
        for col in numeric_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        df.dropna(subset=numeric_cols, inplace=True)
        logger.info(f"ğŸ“Š [è‚¡ç¥¨å®æ—¶æ•°æ®] æ•°æ®æ¸…ç†åå½¢çŠ¶: {df.shape}")
        
        # è‚¡ç¥¨æ¶¨è·Œå¹…å¤„ç†ï¼šè‚¡ç¥¨ç±»å‹ï¼Œç›´æ¥ä½¿ç”¨ï¼ˆå·²ç»æ˜¯ç™¾åˆ†æ¯”å½¢å¼ï¼‰
        if 'æ¶¨è·Œå¹…' in df.columns:
            df['æ¶¨è·Œå¹…'] = pd.to_numeric(df['æ¶¨è·Œå¹…'], errors='coerce')
            # è‚¡ç¥¨æ•°æ®æºè¿”å›ç™¾åˆ†æ¯”å½¢å¼ï¼Œç›´æ¥ä½¿ç”¨
            logger.info("âœ… [è‚¡ç¥¨å®æ—¶æ•°æ®] æ¶¨è·Œå¹…å·²ç»æ˜¯ç™¾åˆ†æ¯”å½¢å¼ï¼Œç›´æ¥ä½¿ç”¨")
        else:
            # å¦‚æœæ²¡æœ‰æ¶¨è·Œå¹…åˆ—ï¼Œåˆ™è®¡ç®—
            df['æ¶¨è·Œå¹…'] = 0.0
            mask = df['æ˜¨æ”¶'] != 0
            df.loc[mask, 'æ¶¨è·Œå¹…'] = ((df.loc[mask, 'æœ€æ–°ä»·'] - df.loc[mask, 'æ˜¨æ”¶']) / df.loc[mask, 'æ˜¨æ”¶']) * 100
        
        logger.info(f"âœ… [è‚¡ç¥¨å®æ—¶æ•°æ®] å¤„ç†å®Œæˆï¼Œæœ€ç»ˆå½¢çŠ¶: {df.shape}")
        return df
    except Exception as e:
        logger.error(f"ğŸ’¥ [è‚¡ç¥¨å®æ—¶æ•°æ®] è·å–å¤±è´¥: {e}", exc_info=True)
        return None

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=2, min=8, max=120))
async def get_stock_daily_history(stock_code: str, data_type: str = "stock"):
    """è·å–å•æ”¯è‚¡ç¥¨çš„å†å²æ—¥çº¿æ•°æ® (å¸¦è‡ªåŠ¨é‡è¯•)"""
    logger.info(f"ğŸ” [è‚¡ç¥¨å†å²æ•°æ®] æ­£åœ¨è·å– {stock_code} çš„å†å²æ—¥çº¿æ•°æ®ï¼Œç±»å‹: {data_type}")
    try:
        # åº”ç”¨æ™ºèƒ½å»¶è¿Ÿæ§åˆ¶
        api_name = f"stock_zh_a_hist_{stock_code}"
        delay = anti_crawling.get_smart_delay(api_name)
        await asyncio.sleep(delay)
        
        # è·å–é…ç½®çš„æ—¥æœŸèŒƒå›´
        start_date, end_date = data_config.get_date_range()
        # è°ƒç”¨è‚¡ç¥¨å†å²æ•°æ®æ¥å£
        # æ•°æ®èŒƒå›´é…ç½®
        
        # ä½¿ç”¨ä¸“é—¨è·å–è‚¡ç¥¨å†å²æ•°æ®çš„æ¥å£
        daily_df = await asyncio.to_thread(
            ak.stock_zh_a_hist,
            symbol=stock_code,
            period="daily",
            adjust="qfq",  # ä½¿ç”¨å‰å¤æƒæ•°æ®
            start_date=start_date,
            end_date=end_date
        )
        
        # è®°å½•æˆåŠŸè¯·æ±‚
        anti_crawling.record_request(api_name)
        logger.info(f"ğŸ“ˆ [è‚¡ç¥¨å†å²æ•°æ®] {stock_code} åŸå§‹æ•°æ®è·å–ç»“æœ: {type(daily_df)}, å½¢çŠ¶: {daily_df.shape if daily_df is not None else 'None'}")
        if daily_df is not None and not daily_df.empty:
            logger.info(f"ğŸ“‹ [è‚¡ç¥¨å†å²æ•°æ®] {stock_code} åŸå§‹åˆ—å: {list(daily_df.columns)}")
            logger.info(f"ğŸ“‹ [è‚¡ç¥¨å†å²æ•°æ®] {stock_code} å‰3è¡Œ:\n{daily_df.head(3)}")
        
        # æ ‡å‡†åŒ–åˆ—å
        if 'æ”¶ç›˜' in daily_df.columns:
            daily_df.rename(columns={'æ”¶ç›˜': 'close'}, inplace=True)
        if 'æœ€é«˜' in daily_df.columns:
            daily_df.rename(columns={'æœ€é«˜': 'high'}, inplace=True)
        if 'æœ€ä½' in daily_df.columns:
            daily_df.rename(columns={'æœ€ä½': 'low'}, inplace=True)
        if 'æ—¥æœŸ' in daily_df.columns:
            daily_df.rename(columns={'æ—¥æœŸ': 'date'}, inplace=True)
        
        logger.info(f"âœ… [è‚¡ç¥¨å†å²æ•°æ®] {stock_code} å¤„ç†å®Œæˆï¼Œæœ€ç»ˆåˆ—å: {list(daily_df.columns) if daily_df is not None else 'None'}")
        return daily_df
    except Exception as e:
        logger.error(f"ğŸ’¥ [è‚¡ç¥¨å†å²æ•°æ®] è·å– {stock_code} æ—¥çº¿æ•°æ®æ—¶å‡ºé”™ (å°†è¿›è¡Œé‡è¯•): {e}", exc_info=True)
        raise e

# ä¸ºäº†å…¼å®¹ç°æœ‰ä»£ç ï¼Œä¿ç•™åŒæ­¥ç‰ˆæœ¬çš„å†å²æ•°æ®è·å–å‡½æ•°
def get_etf_daily_history_sync(etf_code: str, period="daily", adjust=""):
    """è·å–ETFå†å²æ•°æ®ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
    try:
        return ak.fund_etf_hist_em(symbol=etf_code, period=period, adjust=adjust)
    except Exception as e:
        logger.error(f"è·å–ETF {etf_code} å†å²æ•°æ®å¤±è´¥: {e}")
        return None

def get_stock_daily_history_sync(stock_code: str, period="daily", adjust=""):
    """è·å–è‚¡ç¥¨å†å²æ•°æ®ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
    try:
        return ak.stock_zh_a_hist(symbol=stock_code, period=period, adjust=adjust)
    except Exception as e:
        logger.error(f"è·å–è‚¡ç¥¨ {stock_code} å†å²æ•°æ®å¤±è´¥: {e}")
        return None