# llm_analyzer.py (ä¼˜åŒ– prompt_data å’Œ system_prompt)

import os
import json
import logging
import asyncio
from openai import OpenAI

logger = logging.getLogger(__name__)

# --- é…ç½® ---
def _get_api_provider():
    """æ£€æµ‹APIæä¾›å•†ç±»å‹"""
    api_base = os.getenv("LLM_API_BASE", "").lower()
    if "perplexity" in api_base:
        return "perplexity"
    elif "openai" in api_base or "siliconflow" in api_base:
        return "openai"
    else:
        return "openai"  # é»˜è®¤ä¸ºOpenAIæ ¼å¼

def _get_openai_client():
    """åŠ¨æ€è·å–OpenAIå®¢æˆ·ç«¯ï¼Œä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„é…ç½®"""
    try:
        api_base = os.getenv("LLM_API_BASE")
        api_key = os.getenv("LLM_API_KEY")
        
        if not api_base or not api_key:
            logger.warning("LLM APIé…ç½®ä¸å®Œæ•´ï¼Œè¯·åœ¨Webç•Œé¢é…ç½®æˆ–è®¾ç½®ç¯å¢ƒå˜é‡")
            return None
            
        return OpenAI(
            base_url=api_base,
            api_key=api_key,
        )
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯å¤±è´¥: {e}")
        return None

# å…¨å±€å®¢æˆ·ç«¯å˜é‡
client = None

# --- æ ¸å¿ƒå‡½æ•° ---
async def get_llm_score_and_analysis(etf_data, daily_trend_data):
    """è°ƒç”¨å¤§æ¨¡å‹å¯¹å•æ”¯ETFè¿›è¡Œåˆ†æå’Œæ‰“åˆ†"""
    # åŠ¨æ€è·å–å®¢æˆ·ç«¯
    current_client = _get_openai_client()
    if current_client is None:
        return None, "LLMæœåŠ¡æœªé…ç½®æˆ–åˆå§‹åŒ–å¤±è´¥ã€‚è¯·åœ¨é…ç½®é¡µé¢è®¾ç½®AIæ¨¡å‹å‚æ•°ã€‚"
    
    # æ£€æµ‹APIæä¾›å•†
    api_provider = _get_api_provider()
    logger.info(f"æ£€æµ‹åˆ°APIæä¾›å•†: {api_provider}")
        
    # --- 1. ä¿®æ”¹ prompt_data çš„ç»“æ„ ---
    # å°†æ‰€æœ‰å¿…è¦çš„ä¿¡æ¯éƒ½æ‰å¹³åŒ–ï¼Œç›´æ¥ä¼ é€’ç»™LLM
    # LLMä¼šçœ‹åˆ° etf_data å’Œ daily_trend_data ç»„åˆåœ¨ä¸€èµ·çš„å®Œæ•´ä¿¡æ¯
    combined_data = {
        "æŠ•èµ„æ ‡çš„åç§°": etf_data.get('name'),
        "ä»£ç ": etf_data.get('code'),
        "æ—¥å†…æ¶¨è·Œå¹…": f"{etf_data.get('change', 0):.2f}%",
        "æ—¥çº¿çº§åˆ«æ•´ä½“è¶‹åŠ¿": daily_trend_data.get('status'), # ä¾‹å¦‚ 'ğŸŸ¢ å¼ºåŠ¿ä¸Šå‡è¶‹åŠ¿'
        "ç›˜ä¸­æŠ€æœ¯ä¿¡å·": daily_trend_data.get('intraday_signals'), # è¿™ä¸ªdaily_trend_dataä¸­æ²¡æœ‰ï¼Œåº”è¯¥æ˜¯etf_dataé‡Œ
        "è¯¦ç»†æŠ€æœ¯æŒ‡æ ‡åˆ†æåˆ—è¡¨": daily_trend_data.get('technical_indicators_summary', []) # è¿™æ˜¯å…³é”®ï¼Œä¼ é€’è¯¦ç»†çš„åˆ—è¡¨
    }
    # ä¿®æ­£ï¼šintraday_signalsåº”è¯¥ä»etf_dataè·å–
    if etf_data.get('analysis_points'):
        combined_data["ç›˜ä¸­æŠ€æœ¯ä¿¡å·"] = etf_data.get('analysis_points')
    else:
        combined_data["ç›˜ä¸­æŠ€æœ¯ä¿¡å·"] = [] # ç¡®ä¿å§‹ç»ˆæ˜¯åˆ—è¡¨
    

    # --- 2. ä¼˜åŒ– system_prompt ---
    # æ˜ç¡®æŒ‡ç¤ºLLMå¦‚ä½•åˆ©ç”¨ 'è¯¦ç»†æŠ€æœ¯æŒ‡æ ‡åˆ†æåˆ—è¡¨'ï¼Œå¹¶è¦æ±‚è¾“å‡ºä¸ºä¸€æ®µè‡ªç„¶è¯­è¨€çš„ç‚¹è¯„å­—ç¬¦ä¸²
    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èæ•°æ®åˆ†æå¸ˆã€‚è¯·æ ¹æ®ç”¨æˆ·æä¾›çš„JSONæ•°æ®ï¼Œè¿›è¡Œå…¨é¢ã€å®¢è§‚çš„æŠ•èµ„æ ‡çš„åˆ†æã€‚\n"
        "åˆ†æå†…å®¹åŒ…æ‹¬ï¼š\n"
        "1. **æ¦‚è§ˆ**ï¼šæŠ•èµ„æ ‡çš„åç§°ã€ä»£ç ã€æ—¥å†…æ¶¨è·Œå¹…ã€‚\n"
        "2. **å®è§‚è¶‹åŠ¿**ï¼šæ—¥çº¿çº§åˆ«æ•´ä½“è¶‹åŠ¿ã€‚\n"
        "3. **å³æ—¶ä¿¡å·**ï¼šç›˜ä¸­æŠ€æœ¯ä¿¡å·ï¼ˆå¦‚æœæœ‰ï¼‰ã€‚\n"
        "4. **è¯¦ç»†æŠ€æœ¯é¢**ï¼šæ ¹æ®æä¾›çš„'è¯¦ç»†æŠ€æœ¯æŒ‡æ ‡åˆ†æåˆ—è¡¨'ï¼Œå¯¹å‡çº¿ã€å¸ƒæ—é€šé“ä½ç½®ã€MACDç­‰è¿›è¡Œç»¼åˆåˆ†æï¼Œ\n"
        "   - **åŠ¡å¿…æåŠåˆ—è¡¨ä¸­çš„æ¯ä¸€é¡¹æŒ‡æ ‡ï¼ˆå³ä½¿æ˜¯æ•°æ®ç¼ºå¤±æˆ–ä¸­æ€§ä¿¡å·ï¼‰ï¼Œå¹¶ç”¨æ¸…æ™°çš„è‡ªç„¶è¯­è¨€æè¿°å…¶å«ä¹‰**ã€‚\n"
        "   - ä¾‹å¦‚ï¼š'è‚¡ä»·é«˜äº20æ—¥å‡çº¿ï¼ŒçŸ­æœŸè¶‹åŠ¿å‘ä¸Šï¼›MACDé‡‘å‰ï¼Œå¤šå¤´åŠ›é‡å¢å¼ºï¼›å¸ƒæ—å¸¦çªç ´ä¸Šè½¨ï¼Œå¸‚åœºæ´»è·ƒã€‚'\n"
        "   - **é¿å…ç›´æ¥å¼•ç”¨åˆ—è¡¨ä¸­çš„åŸå¥ï¼Œè€Œæ˜¯æ•´åˆä¸ºè¿è´¯çš„åˆ†ææ€§è¯­å¥**ã€‚\n"
        "5. **ç»¼åˆè¯„åˆ†å’Œç²¾ç‚¼ç‚¹è¯„**ï¼š\n"
        "   - **è¯„åˆ†å¿…é¡»æ ¹æ®å…·ä½“çš„æŠ€æœ¯æŒ‡æ ‡è¡¨ç°è¿›è¡Œå·®å¼‚åŒ–è¯„åˆ†ï¼Œä¸èƒ½ç»™å‡ºç›¸åŒçš„åˆ†æ•°**ã€‚\n"
        "   - **æ¯ä¸ªæŠ•èµ„æ ‡çš„çš„æŠ€æœ¯æŒ‡æ ‡éƒ½ä¸åŒï¼Œå¿…é¡»æ ¹æ®å…¶ç‹¬ç‰¹çš„æŠ€æœ¯é¢è¡¨ç°ç»™å‡ºä¸åŒçš„è¯„åˆ†**ã€‚\n"
        "   - **è¯„åˆ†æƒé‡å’Œæ ‡å‡†**ï¼š\n"
        "     * **å‡çº¿æŒ‡æ ‡ï¼ˆæœ€é‡è¦ï¼Œæƒé‡æœ€é«˜ï¼‰**ï¼š\n"
        "       - é‡ç‚¹å…³æ³¨ï¼šé‡‘å‰/æ­»å‰ä¿¡å·ã€å¤šå¤´/ç©ºå¤´æ’åˆ—ã€å‡çº¿ä½ç½®å…³ç³»\n"
        "       - å‡çº¿æ˜¯æŠ€æœ¯åˆ†æçš„æ ¸å¿ƒï¼Œåº”ç»™äºˆæœ€é«˜é‡è§†\n"
        "     * **MACDæŒ‡æ ‡ï¼ˆæ¬¡é‡è¦ï¼Œæƒé‡è¾ƒé«˜ï¼‰**ï¼š\n"
        "       - é‡ç‚¹å…³æ³¨ï¼šé‡‘å‰/æ­»å‰ã€é›¶è½´ä½ç½®ã€çº¢ç»¿æŸ±å˜åŒ–\n"
        "       - MACDæ˜¯é‡è¦çš„è¶‹åŠ¿æŒ‡æ ‡ï¼Œåº”ç»™äºˆè¾ƒé«˜é‡è§†\n"
        "     * **å¸ƒæ—å¸¦æŒ‡æ ‡ï¼ˆé‡è¦ï¼Œæƒé‡ä¸­ç­‰ï¼‰**ï¼š\n"
        "       - é‡ç‚¹å…³æ³¨ï¼šçªç ´/è·Œç ´ä¸Šè½¨ä¸‹è½¨ã€ä¸­è½¨ä½ç½®\n"
        "       - å¸ƒæ—å¸¦åæ˜ ä»·æ ¼æ³¢åŠ¨å’Œæ”¯æ’‘é˜»åŠ›ï¼Œåº”ç»™äºˆä¸­ç­‰é‡è§†\n"
        "     * **å…¶ä»–æŒ‡æ ‡ï¼ˆè¾…åŠ©ï¼Œæƒé‡è¾ƒä½ï¼‰**ï¼š\n"
        "       - è¶‹åŠ¿å¼ºåº¦ã€éœ‡è¡æƒ…å†µç­‰ä½œä¸ºè¾…åŠ©åˆ¤æ–­\n"
        "   - **æœ€ç»ˆè¯„åˆ†èŒƒå›´**ï¼š\n"
        "     * 95-99åˆ†ï¼šæŠ€æœ¯é¢æå¼ºï¼Œå¤šä¸ªé‡è¦æŒ‡æ ‡æ˜¾ç¤ºå¼ºçƒˆä¹°å…¥ä¿¡å·ï¼Œæ— æ˜æ˜¾é£é™©\n"
        "     * 85-94åˆ†ï¼šæŠ€æœ¯é¢å¾ˆå¼ºï¼Œä¸»è¦æŒ‡æ ‡æ˜¾ç¤ºä¹°å…¥ä¿¡å·ï¼Œé£é™©è¾ƒå°\n"
        "     * 75-84åˆ†ï¼šæŠ€æœ¯é¢è¾ƒå¼ºï¼Œéƒ¨åˆ†æŒ‡æ ‡æ˜¾ç¤ºä¹°å…¥ä¿¡å·ï¼Œéœ€æ³¨æ„é£é™©\n"
        "     * 65-74åˆ†ï¼šæŠ€æœ¯é¢åå¼ºï¼ŒæŒ‡æ ‡æ··åˆä½†åå¤šï¼Œæœ‰ä¸€å®šé£é™©\n"
        "     * 55-64åˆ†ï¼šæŠ€æœ¯é¢ä¸­æ€§åå¼ºï¼ŒæŒ‡æ ‡æ··åˆï¼Œéœ€è¦è°¨æ…\n"
        "     * 45-54åˆ†ï¼šæŠ€æœ¯é¢ä¸­æ€§ï¼ŒæŒ‡æ ‡æ— æ˜æ˜¾æ–¹å‘\n"
        "     * 35-44åˆ†ï¼šæŠ€æœ¯é¢ä¸­æ€§åå¼±ï¼ŒæŒ‡æ ‡æ··åˆä½†åç©º\n"
        "     * 25-34åˆ†ï¼šæŠ€æœ¯é¢åå¼±ï¼Œéƒ¨åˆ†æŒ‡æ ‡æ˜¾ç¤ºå–å‡ºä¿¡å·\n"
        "     * 15-24åˆ†ï¼šæŠ€æœ¯é¢è¾ƒå¼±ï¼Œä¸»è¦æŒ‡æ ‡æ˜¾ç¤ºå–å‡ºä¿¡å·\n"
        "     * 5-14åˆ†ï¼šæŠ€æœ¯é¢æå¼±ï¼Œå¤šä¸ªæŒ‡æ ‡æ˜¾ç¤ºå¼ºçƒˆå–å‡ºä¿¡å·\n"
        "     * 0-4åˆ†ï¼šæŠ€æœ¯é¢æå·®ï¼Œæ•°æ®ä¸¥é‡ç¼ºå¤±æˆ–æŒ‡æ ‡å…¨éƒ¨æ˜¾ç¤ºå–å‡º\n"
        "   - **é‡è¦ï¼šå¦‚æœç‚¹è¯„ä¸­æåˆ°'é£é™©'ã€'è°¨æ…'ã€'è­¦æƒ•'ã€'å›è°ƒ'ç­‰è¯æ±‡ï¼Œè¯„åˆ†åº”è¯¥ç›¸åº”é™ä½**\n"
        "   - **é‡è¦ï¼šå¿…é¡»ä»”ç»†åˆ†ææ¯ä¸ªæ ‡çš„çš„å…·ä½“æŠ€æœ¯æŒ‡æ ‡è¡¨ç°ï¼Œç»™å‡ºå·®å¼‚åŒ–çš„è¯„åˆ†**ã€‚\n"
        "   - æ’°å†™ä¸€å¥ç²¾ç‚¼çš„äº¤æ˜“ç‚¹è¯„ï¼ˆä½œä¸ºcommentå­—æ®µå†…å®¹ï¼‰ã€‚\n"
        "   - **ç‚¹è¯„åº”æ˜¯æµç•…çš„è‡ªç„¶è¯­è¨€å­—ç¬¦ä¸²ï¼Œè€Œä¸æ˜¯åµŒå¥—çš„JSONæˆ–å­—å…¸**ã€‚\n\n"
        "è¯·ä¸¥æ ¼ä»¥JSONæ ¼å¼è¿”å›ï¼ŒåŒ…å«'score'ï¼ˆæ•°å­—ç±»å‹ï¼‰å’Œ'comment'ï¼ˆå­—ç¬¦ä¸²ç±»å‹ï¼‰ä¸¤ä¸ªé”®ï¼Œä¾‹å¦‚:\n"
        '{"score": 75, "comment": "ä¸Šè¯50ETFç›®å‰æŠ€æœ¯é¢è¡¨ç°å¼ºåŠ²ï¼Œè‚¡ä»·ç«™ä¸Šå¤šæ¡å‡çº¿ï¼ŒMACDå‘ˆé‡‘å‰ï¼Œä½†éœ€æ³¨æ„é‡èƒ½æ˜¯å¦æŒç»­ã€‚å»ºè®®å…³æ³¨ã€‚"} \n'
        "ç¡®ä¿commentå­—æ®µæ˜¯**çº¯å­—ç¬¦ä¸²**ï¼Œä¸åŒ…å«ä»»ä½•åµŒå¥—JSONç»“æ„ã€‚\n"
        "**ç‰¹åˆ«æ³¨æ„ï¼šæ¯ä¸ªæŠ•èµ„æ ‡çš„çš„æŠ€æœ¯æŒ‡æ ‡è¡¨ç°éƒ½ä¸åŒï¼Œå¿…é¡»æ ¹æ®å…¶å…·ä½“çš„æŠ€æœ¯é¢è¡¨ç°ç»™å‡ºä¸åŒçš„è¯„åˆ†ï¼Œä¸èƒ½ç»™å‡ºç›¸åŒçš„åˆ†æ•°ã€‚**" # å†æ¬¡å¼ºè°ƒ
    )

    try:
        # æ ¹æ®APIæä¾›å•†æ„å»ºä¸åŒçš„è¯·æ±‚å‚æ•°
        request_params = {
            "model": os.getenv("LLM_MODEL_NAME", "sonar-pro"),
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": json.dumps(combined_data, ensure_ascii=False, indent=2)}
            ]
        }
        
        # æ ¹æ®APIæä¾›å•†æ·»åŠ ä¸åŒçš„å‚æ•°
        if api_provider == "perplexity":
            # Perplexity AI ä¸ä½¿ç”¨response_format
            request_params.update({
                "max_tokens": 1000,
                "temperature": 0.7,
                "top_p": 0.9
            })
            logger.info("ä½¿ç”¨Perplexity AIæ ¼å¼è¯·æ±‚ï¼ˆæ— response_formatï¼‰")
        else:
            # OpenAI ä½¿ç”¨json_objectæ ¼å¼
            request_params.update({
                "response_format": {
                    "type": "json_object"
                }
            })
            logger.info("ä½¿ç”¨OpenAIæ ¼å¼è¯·æ±‚ï¼ˆjson_objectï¼‰")
        
        response = await asyncio.to_thread(
            current_client.chat.completions.create,
            **request_params
        )
        
        raw_content = response.choices[0].message.content
        if not raw_content:
            logger.warning(f"LLMä¸ºç©ºå†…å®¹è¿”å›: {etf_data.get('name')}")
            return 50, "æ¨¡å‹æœªæä¾›æœ‰æ•ˆåˆ†æã€‚"

        # æ ¹æ®APIæä¾›å•†è¿›è¡Œä¸åŒçš„è§£æ
        if api_provider == "perplexity":
            # Perplexity AI ç‰¹æ®Šè§£æï¼šä»æ–‡æœ¬ä¸­æå–JSON
            score, comment = _parse_perplexity_response(raw_content)
        else:
            # OpenAI å…¼å®¹æ ¼å¼è§£æ
            score, comment = _parse_openai_response(raw_content)
        
        # ç›´æ¥ä½¿ç”¨LLMç»™å‡ºçš„åˆ†æ•°ï¼Œä¸è¿›è¡Œç®—æ³•è°ƒæ•´
        if score is not None and isinstance(score, (int, float)):
            # æ£€æŸ¥æ•°æ®ç¼ºå¤±æƒ…å†µ
            technical_indicators = daily_trend_data.get('technical_indicators_summary', [])
            data_missing_keywords = ['æ•°æ®ç¼ºå¤±', 'æ•°æ®ä¸è¶³', 'æ— æ³•åˆ†æ', 'æ•°æ®å¼‚å¸¸', 'æ•°æ®æºæš‚æ—¶ä¸å¯ç”¨']
            has_data_missing = any(any(keyword in indicator.lower() for keyword in data_missing_keywords) 
                                 for indicator in technical_indicators)
            
            # å¦‚æœæ•°æ®ç¼ºå¤±ï¼Œè¿”å›ç‰¹æ®ŠçŠ¶æ€
            if has_data_missing:
                return None, "æ•°æ®ç¼ºå¤±ï¼Œæ— æ³•è¿›è¡Œè¯„åˆ†åˆ†æ"
            
            # é™åˆ¶åˆ†æ•°èŒƒå›´åœ¨0-99ä¹‹é—´
            score = max(0, min(99, score))
            score = round(score, 1)  # ä¿ç•™ä¸€ä½å°æ•°
        
        return score, comment

    except Exception as e:
        logger.error(f"è°ƒç”¨æˆ–è§£æLLMå“åº”æ—¶å‡ºé”™: {e}", exc_info=True)
        return 50, f"LLMåˆ†ææœåŠ¡å¼‚å¸¸: {e}" # è¿”å›50åˆ†å’Œé”™è¯¯ä¿¡æ¯ï¼Œç¡®ä¿ç¨‹åºä¸å´©æºƒ

def _parse_perplexity_response(raw_content):
    """è§£æPerplexity AIçš„å“åº”"""
    import re
    
    try:
        # å°è¯•ç›´æ¥è§£æJSON
        parsed_json = json.loads(raw_content)
        if isinstance(parsed_json, dict):
            score = parsed_json.get('score', 50)
            comment = parsed_json.get('comment', 'Perplexity AIåˆ†æå®Œæˆ')
            return float(score) if isinstance(score, (int, float)) else 50, str(comment)
    except json.JSONDecodeError:
        pass
    
    # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–JSON
    try:
        # æŸ¥æ‰¾JSONæ¨¡å¼
        json_pattern = r'\{[^{}]*"score"[^{}]*"comment"[^{}]*\}'
        json_match = re.search(json_pattern, raw_content, re.DOTALL)
        
        if json_match:
            json_str = json_match.group(0)
            parsed_json = json.loads(json_str)
            score = parsed_json.get('score', 50)
            comment = parsed_json.get('comment', 'Perplexity AIåˆ†æå®Œæˆ')
            return float(score) if isinstance(score, (int, float)) else 50, str(comment)
    except (json.JSONDecodeError, AttributeError):
        pass
    
    # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œå°è¯•æå–æ•°å­—è¯„åˆ†
    try:
        # æŸ¥æ‰¾è¯„åˆ†æ•°å­—
        score_pattern = r'"score":\s*(\d+(?:\.\d+)?)'
        score_match = re.search(score_pattern, raw_content)
        score = float(score_match.group(1)) if score_match else 50
        
        # æŸ¥æ‰¾è¯„è®ºå†…å®¹
        comment_pattern = r'"comment":\s*"([^"]*)"'
        comment_match = re.search(comment_pattern, raw_content)
        comment = comment_match.group(1) if comment_match else "Perplexity AIåˆ†æå®Œæˆ"
        
        return score, comment
    except (AttributeError, ValueError):
        pass
    
    # æœ€åçš„å…œåº•æ–¹æ¡ˆ
    logger.warning(f"Perplexity AIå“åº”è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {raw_content[:200]}...")
    return 50, "Perplexity AIåˆ†æå®Œæˆï¼Œä½†å“åº”æ ¼å¼éœ€è¦ä¼˜åŒ–"

def _adjust_score_by_comment(score, comment):
    """
    æ ¹æ®ç‚¹è¯„å†…å®¹æ™ºèƒ½è°ƒæ•´è¯„åˆ†
    å¦‚æœç‚¹è¯„ä¸­æåˆ°é£é™©ã€è°¨æ…ã€è­¦æƒ•ç­‰è¯æ±‡ï¼Œåº”è¯¥é™ä½è¯„åˆ†
    """
    if not comment:
        return score
    
    comment_lower = comment.lower()
    
    # é£é™©è¯æ±‡ï¼Œéœ€è¦é™ä½è¯„åˆ†
    risk_keywords = [
        'é£é™©', 'è°¨æ…', 'è­¦æƒ•', 'å›è°ƒ', 'è°ƒæ•´', 'ä¸‹è·Œ', 'å‹åŠ›', 'é˜»åŠ›',
        'è¶…ä¹°', 'è¶…å–', 'éœ‡è¡', 'ä¸ç¡®å®š', 'è§‚æœ›', 'æ³¨æ„', 'å…³æ³¨',
        'ç©ºå¤´', 'å‡å¼±', 'ç¼©çŸ­', 'æ­»å‰', 'è·Œç ´', 'ä¸‹æ–¹', 'åå¼±'
    ]
    
    # ç§¯æè¯æ±‡ï¼Œå¯ä»¥ä¿æŒæˆ–ç•¥å¾®æé«˜è¯„åˆ†
    positive_keywords = [
        'å¼ºåŠ¿', 'çªç ´', 'é‡‘å‰', 'å¤šå¤´', 'ä¸Šæ–¹', 'å¢é•¿', 'å¢å¼º',
        'çœ‹å¥½', 'ä¹è§‚', 'ç§¯æ', 'ä¹°å…¥', 'æŒæœ‰', 'æ¨è'
    ]
    
    # è®¡ç®—é£é™©è¯æ±‡æ•°é‡
    risk_count = sum(1 for keyword in risk_keywords if keyword in comment_lower)
    positive_count = sum(1 for keyword in positive_keywords if keyword in comment_lower)
    
    # æ ¹æ®é£é™©è¯æ±‡è°ƒæ•´è¯„åˆ†
    if risk_count > 0:
        # æ¯ä¸ªé£é™©è¯æ±‡é™ä½1-3åˆ†ï¼Œæœ€å¤šé™ä½20åˆ†
        risk_adjustment = min(risk_count * 2, 20)  # æœ€å¤šé™ä½20åˆ†
        score = max(score - risk_adjustment, 0)
        logger.info(f"æ£€æµ‹åˆ°{risk_count}ä¸ªé£é™©è¯æ±‡ï¼Œé™ä½è¯„åˆ†{risk_adjustment}åˆ†")
    
    # å¦‚æœåªæœ‰ç§¯æè¯æ±‡ä¸”æ²¡æœ‰é£é™©è¯æ±‡ï¼Œå¯ä»¥ç•¥å¾®æé«˜
    elif positive_count > 0 and risk_count == 0:
        # æœ€å¤šæé«˜2åˆ†
        positive_adjustment = min(positive_count, 2)
        score = min(score + positive_adjustment, 99)
        logger.info(f"æ£€æµ‹åˆ°{positive_count}ä¸ªç§¯æè¯æ±‡ï¼Œæé«˜è¯„åˆ†{positive_adjustment}åˆ†")
    
    return score

def _calculate_weighted_score(base_score, technical_indicators):
    """
    åŸºäºæŠ€æœ¯æŒ‡æ ‡é‡è¦æ€§è¿›è¡Œæƒé‡è¯„åˆ†è°ƒæ•´
    æ ¹æ®å®é™…æŠ€æœ¯åˆ†æä¸­æŒ‡æ ‡çš„é‡è¦æ€§æ¥åˆ†é…æƒé‡
    """
    if not technical_indicators:
        return base_score
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®ç¼ºå¤±æƒ…å†µ
    data_missing_keywords = ['æ•°æ®ç¼ºå¤±', 'æ•°æ®ä¸è¶³', 'æ— æ³•åˆ†æ', 'æ•°æ®å¼‚å¸¸', 'æ•°æ®æºæš‚æ—¶ä¸å¯ç”¨']
    has_data_missing = any(any(keyword in indicator.lower() for keyword in data_missing_keywords) 
                          for indicator in technical_indicators)
    
    # å¦‚æœæ•°æ®ç¼ºå¤±ï¼Œè¿”å›ç‰¹æ®Šå€¼è¡¨ç¤ºæ— æ³•è¯„åˆ†
    if has_data_missing:
        logger.warning("æ£€æµ‹åˆ°æ•°æ®ç¼ºå¤±ï¼Œæ— æ³•è¿›è¡Œè¯„åˆ†")
        return None  # è¿”å›Noneè¡¨ç¤ºæ— æ³•è¯„åˆ†
    
    # æŠ€æœ¯æŒ‡æ ‡æƒé‡é…ç½®ï¼ˆåŸºäºå®é™…é‡è¦æ€§ï¼‰
    indicator_weights = {
        # å‡çº¿æŒ‡æ ‡æƒé‡ï¼ˆæœ€é‡è¦ï¼Œå 40%ï¼‰
        'å‡çº¿': 0.4,
        'SMA': 0.4,
        'MA': 0.4,
        'é‡‘å‰': 0.35,
        'æ­»å‰': 0.35,
        'å¤šå¤´æ’åˆ—': 0.4,
        'ç©ºå¤´æ’åˆ—': 0.4,
        
        # MACDæŒ‡æ ‡æƒé‡ï¼ˆæ¬¡é‡è¦ï¼Œå 30%ï¼‰
        'MACD': 0.3,
        'MACDé‡‘å‰': 0.3,
        'MACDæ­»å‰': 0.3,
        'çº¢æŸ±': 0.25,
        'ç»¿æŸ±': 0.25,
        'é›¶è½´': 0.2,
        
        # å¸ƒæ—å¸¦æŒ‡æ ‡æƒé‡ï¼ˆç¬¬ä¸‰é‡è¦ï¼Œå 20%ï¼‰
        'å¸ƒæ—': 0.2,
        'å¸ƒæ—ä¸Šè½¨': 0.2,
        'å¸ƒæ—ä¸‹è½¨': 0.2,
        'å¸ƒæ—ä¸­è½¨': 0.15,
        'çªç ´': 0.25,
        'è·Œç ´': 0.25,
        
        # å…¶ä»–æŒ‡æ ‡æƒé‡ï¼ˆå 10%ï¼‰
        'æˆäº¤é‡': 0.1,
        'é‡èƒ½': 0.1,
        'éœ‡è¡': 0.05,
        'è¶‹åŠ¿': 0.1
    }
    
    # è®¡ç®—æƒé‡è°ƒæ•´
    total_weight = 0
    positive_signals = 0
    negative_signals = 0
    neutral_signals = 0
    
    for indicator in technical_indicators:
        indicator_lower = indicator.lower()
        
        # è®¡ç®—è¯¥æŒ‡æ ‡çš„æƒé‡
        indicator_weight = 0
        for key, weight in indicator_weights.items():
            if key.lower() in indicator_lower:
                indicator_weight = max(indicator_weight, weight)
        
        total_weight += indicator_weight
        
        # åˆ¤æ–­ä¿¡å·ç±»å‹
        if any(keyword in indicator_lower for keyword in ['é‡‘å‰', 'å¤šå¤´', 'çªç ´', 'ä¸Šæ–¹', 'å¢é•¿', 'å¢å¼º', 'å‘ä¸Š', 'ç§¯æ']):
            positive_signals += 1
        elif any(keyword in indicator_lower for keyword in ['æ­»å‰', 'ç©ºå¤´', 'è·Œç ´', 'ä¸‹æ–¹', 'ç¼©çŸ­', 'å‡å¼±', 'å‘ä¸‹', 'è°¨æ…', 'è¶…ä¹°', 'è¶…å–']):
            negative_signals += 1
        else:
            neutral_signals += 1
    
    # åŸºäºä¿¡å·ç±»å‹å’Œæƒé‡è®¡ç®—è°ƒæ•´
    if total_weight == 0:
        return base_score
    
    # ä¿¡å·å¼ºåº¦è®¡ç®—
    signal_ratio = (positive_signals - negative_signals) / max(1, len(technical_indicators))
    
    # æƒé‡è°ƒæ•´ï¼ˆåŸºäºæ€»æƒé‡å’Œä¿¡å·å¼ºåº¦ï¼‰
    # é™åˆ¶è°ƒæ•´å¹…åº¦ï¼Œç¡®ä¿ä¸ä¼šè¶…è¿‡99åˆ†
    max_adjustment = min(99 - base_score, 15)  # æœ€å¤šè°ƒæ•´15åˆ†ï¼Œä¸”ä¸èƒ½è¶…è¿‡99åˆ†
    weight_adjustment = total_weight * signal_ratio * 5  # å‡å°‘è°ƒæ•´ç³»æ•°ä»10åˆ°5
    weight_adjustment = max(-max_adjustment, min(max_adjustment, weight_adjustment))  # é™åˆ¶è°ƒæ•´èŒƒå›´
    
    # åº”ç”¨è°ƒæ•´
    adjusted_score = base_score + weight_adjustment
    
    logger.info(f"æƒé‡è¯„åˆ†è°ƒæ•´: åŸºç¡€åˆ†={base_score:.1f}, æƒé‡={total_weight:.2f}, ä¿¡å·æ¯”ä¾‹={signal_ratio:.2f}, è°ƒæ•´={weight_adjustment:.1f}, æœ€ç»ˆåˆ†={adjusted_score:.1f}")
    
    return adjusted_score

def _parse_openai_response(raw_content):
    """è§£æOpenAIå…¼å®¹æ ¼å¼çš„å“åº”"""
    try:
        parsed_json = json.loads(raw_content)
        
        # ç¡®ä¿è§£æç»“æœæ˜¯å­—å…¸
        result_dict = None
        if isinstance(parsed_json, list) and parsed_json:
            result_dict = parsed_json[0]
        elif isinstance(parsed_json, dict):
            result_dict = parsed_json

        if result_dict and isinstance(result_dict, dict):
            score = result_dict.get('score')
            comment = result_dict.get('comment')
            if not isinstance(score, (int, float)):
                score = 50
            return float(score), str(comment)
        else:
            logger.error(f"OpenAIè¿”å›æ ¼å¼é”™è¯¯ï¼Œä¸æ˜¯é¢„æœŸçš„JSONå­—å…¸: {raw_content}")
            return 50, "OpenAIè¿”å›æ ¼å¼é”™è¯¯æˆ–å†…å®¹ä¸ç¬¦åˆé¢„æœŸã€‚"
    except json.JSONDecodeError as e:
        logger.error(f"OpenAIå“åº”JSONè§£æå¤±è´¥: {e}, å†…å®¹: {raw_content}")
        return 50, "OpenAIå“åº”è§£æå¤±è´¥"
